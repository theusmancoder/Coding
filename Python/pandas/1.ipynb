{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\UNB\\\\Programs\\\\scrapenovels.py\\\\Novels excel files\\\\kitabnagridotcom\\\\merge.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m file = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mD:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUNB\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mPrograms\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mscrapenovels.py\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mNovels excel files\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mkitabnagridotcom\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mmerge.xlsx\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PCS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[32m    494\u001b[39m     should_close = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     io = \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine != io.engine:\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PCS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[39m, in \u001b[36mExcelFile.__init__\u001b[39m\u001b[34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   1548\u001b[39m     ext = \u001b[33m\"\u001b[39m\u001b[33mxls\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     ext = \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m   1552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1554\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1555\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mExcel file format cannot be determined, you must specify \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1556\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33man engine manually.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1557\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PCS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[39m, in \u001b[36minspect_excel_format\u001b[39m\u001b[34m(content_or_path, storage_options)\u001b[39m\n\u001b[32m   1399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m   1400\u001b[39m     content_or_path = BytesIO(content_or_path)\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m   1404\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[32m   1405\u001b[39m     stream = handle.handle\n\u001b[32m   1406\u001b[39m     stream.seek(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PCS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'D:\\\\UNB\\\\Programs\\\\scrapenovels.py\\\\Novels excel files\\\\kitabnagridotcom\\\\merge.xlsx'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file = r\"D:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\kitabnagridotcom\\merge.xlsx\"\n",
    "data = pd.read_excel(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas ka display option badhao\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)  # Sabhi rows show hongi\n",
    "pd.set_option('display.max_columns', None)  # Sabhi columns bhi show hongay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.tolist())  # Ye sab column names ko list me print karega\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data.Title ‚Üí Dot Notation (.)\n",
    "Agar aap data.Title likh rahe hain, to iska bhi same matlab hai:\n",
    "üîπ Ye shortcut syntax hai data['Title'] ka, lekin har column ke liye kaam nahi karta.\n",
    "\n",
    "Example:\n",
    "print(data.Title)  # Same as data['Title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kab Avoid Karna Chahiye?\n",
    "üö® Agar column ka naam space ya special character contain karta ho, to dot notation (data.Title) error de sakti hai:\n",
    "\n",
    "data[\"Book Title\"]  # ‚úÖ Works\n",
    "data.Book Title  # ‚ùå Error (Space in column name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data['Title'] () ‚Üí Brackets (())\n",
    "Agar aap data['Title'] () likh rahe hain, to iska matlab hai Title column pe kisi function ka apply hona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Title\"] = data[\"Title\"].apply(lambda x: x.upper())\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['Title','Google Drive Links']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['Title','Google Drive Links']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[[1,5,6,7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:,[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = data['Title']=='Ishq ki Sargazisht Novel Complete Free Pdf By Reem Farooq'\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = data[mask]  # Returns rows where \"Title\" is \"namal\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = data[data['Title'] == 'pdf']\n",
    "print(filtered_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = data[data['Title'].str.lower().str.contains('pdf', na=False)]\n",
    "print(filtered_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "display(filtered_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.to_excel(\"filtered_titles.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Google Drive Links'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Google Drive Links'].value_counts().head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_titles = \" \".join(df['Title']).lower().split()  # Saare words ek list me\n",
    "word_freq = Counter(all_titles)  # Word count\n",
    "print(word_freq.most_common(20))  # Top 20 common words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[df['Title'].str.contains(\"umera\", case=False, na=False)]\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.tolist())  # Ye sab column names ko list me print karega\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter Notebook me ye sab variables show karega\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aap ke kehne ke mutabiq, agar Title column mein kahin bhi \"Download\", \"pdf\", ya dono shamil hoon, to wo words hata diye jayen lekin baaki title waise ke waise rahen ‚Äî to yahaan ek Python code diya gaya hai jo pandas ka use karta hai aur result ko ek naye Excel file mein save karta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Excel file saved as: E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\Besturdubooks_ok\\ready.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Original Excel file ka path\n",
    "input_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\Besturdubooks_ok\\ok1.xlsx\"  # Replace this with your actual filename\n",
    "output_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\Besturdubooks_ok\\ready.xlsx\"  # Naya file yahi naam se banega\n",
    "\n",
    "# Excel file read karo\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Title column mein se 'download' aur 'pdf' hatao (case insensitive)\n",
    "def clean_title(title):\n",
    "    return re.sub(r'\\b(download|pdf)\\b', '', str(title), flags=re.IGNORECASE).strip()\n",
    "\n",
    "df['Title'] = df['Title'].apply(clean_title)\n",
    "\n",
    "# Nayi file save karo\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"Cleaned Excel file saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediafire link se title extract karna ‚Äî jaise:\n",
    "\n",
    "bash\n",
    "Copy\n",
    "Edit\n",
    "https://www.mediafire.com/file/8chlaul1wvevvsj/Jo+chale+to+jan+se+guzar+gaey-min.pdf/file\n",
    "Se title nikaalna:\n",
    "\n",
    "arduino\n",
    "Copy\n",
    "Edit\n",
    "Jo chale to jan se guzar gaey-min.pdf\n",
    "Phir Title column ke andar is title ko replace karna (agar Mediafire Links column mein koi link ho), aur phir result ko new Excel file mein save karna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Excel file saved as: E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\digestkahani_ok\\ready.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from urllib.parse import unquote\n",
    "\n",
    "# Original Excel file\n",
    "input_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\digestkahani_ok\\ok3.xlsx\" # Replace with your file name\n",
    "output_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\digestkahani_ok\\ready.xlsx\"\n",
    "\n",
    "# Load the file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Function to extract title from Mediafire link\n",
    "def extract_title_from_link(link):\n",
    "    if isinstance(link, str) and 'mediafire.com' in link:\n",
    "        match = re.search(r'/file/.+?/([^/]+)', link)\n",
    "        if match:\n",
    "            filename = match.group(1)\n",
    "            filename = unquote(filename.replace('+', ' '))  # Decode URL and replace + with space\n",
    "            return filename\n",
    "    return None\n",
    "\n",
    "# Replace Title where Mediafire link exists\n",
    "for idx, row in df.iterrows():\n",
    "    mediafire_link = row.get(\"Mediafire Links\", \"\")\n",
    "    extracted_title = extract_title_from_link(mediafire_link)\n",
    "    if extracted_title:\n",
    "        df.at[idx, \"Title\"] = extracted_title\n",
    "\n",
    "# Save to new Excel file\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"New Excel file saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Aap chahte hain ke:\n",
    "\n",
    "Title column mein agar kahin bhi .pdf likha ho, to usse remove kar diya jaye.\n",
    "\n",
    "Ye kaam Mediafire link se title extract karne ke baad bhi apply ho.\n",
    "\n",
    "Yeh raha final Python code jo sab kuch karta hai:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ New Excel file saved as: E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\digestkahani_ok\\ready1.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from urllib.parse import unquote\n",
    "\n",
    "# Original Excel file\n",
    "input_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\digestkahani_ok\\ready.xlsx\" # ‚Üê replace with your file name\n",
    "output_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\digestkahani_ok\\ready1.xlsx\"\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Function to extract and clean title from Mediafire link\n",
    "def extract_title_from_link(link):\n",
    "    if isinstance(link, str) and 'mediafire.com' in link:\n",
    "        match = re.search(r'/file/.+?/([^/]+)', link)\n",
    "        if match:\n",
    "            filename = match.group(1)\n",
    "            filename = unquote(filename.replace('+', ' '))  # Convert + to space and decode\n",
    "            filename = filename.replace('.pdf', '').strip()  # Remove .pdf from title\n",
    "            return filename\n",
    "    return None\n",
    "\n",
    "# Update Title column with extracted title from Mediafire Links\n",
    "for idx, row in df.iterrows():\n",
    "    mediafire_link = row.get(\"Mediafire Links\", \"\")\n",
    "    extracted_title = extract_title_from_link(mediafire_link)\n",
    "    if extracted_title:\n",
    "        df.at[idx, \"Title\"] = extracted_title\n",
    "\n",
    "# Also remove '.pdf' from existing Title values (just in case)\n",
    "df['Title'] = df['Title'].str.replace('.pdf', '', case=False).str.strip()\n",
    "\n",
    "# Save to new Excel file\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"‚úÖ New Excel file saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title column mein jahan jahan .pdf - Google Drive likha ho, usay remove kar diya jaye.\n",
    "\n",
    "Aur result usi folder mein new Excel file ke taur par save ho jaye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned file saved as: E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\DigestLibrarydotcom_ok\\ready.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Input aur output file ka naam\n",
    "input_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\DigestLibrarydotcom_ok\\merged_output.xlsx\"# ‚Üê yahan apni file ka naam daalein\n",
    "output_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\DigestLibrarydotcom_ok\\ready.xlsx\"\n",
    "\n",
    "# Excel file read karo\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# 'Title' column mein '.pdf - Google Drive' ko remove karo\n",
    "df['Title'] = df['Title'].str.replace('.pdf - Google Drive', '', case=False).str.strip()\n",
    "\n",
    "# Nayi file save karo\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"‚úÖ Cleaned file saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title column mein jahan jahan \"free pdf download\" likha ho (case-insensitive), usay remove kar diya jaye.\n",
    "\n",
    "Baaki title bilkul waise ka waise rahe.\n",
    "\n",
    "Naya Excel file save ho jaye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Words removed and file saved as: E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\freepdf_ok\\ready.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Step 1: File names\n",
    "input_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\freepdf_ok\\ok7.xlsx\"  # ‚Üê replace with your actual file name\n",
    "output_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\freepdf_ok\\ready.xlsx\"\n",
    "\n",
    "# Step 2: Load Excel file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Step 3: Function to clean unwanted words from title\n",
    "def clean_title(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove words: free, download, online (case-insensitive)\n",
    "        cleaned = re.sub(r'\\b(free|download|online)\\b', '', text, flags=re.IGNORECASE)\n",
    "        return re.sub(r'\\s+', ' ', cleaned).strip()  # Clean extra spaces\n",
    "    return text\n",
    "\n",
    "# Step 4: Apply function to 'Title' column\n",
    "df['Title'] = df['Title'].apply(clean_title)\n",
    "\n",
    "# Step 5: Save cleaned DataFrame\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"‚úÖ Words removed and file saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title column mein jahan jahan bhi dot (.) ho, usay remove kar diya jaye.\n",
    "\n",
    "Baaki text bilkul same rahe.\n",
    "\n",
    "Aur result ek naye Excel file mein save ho jaye.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dots removed and file saved as: E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\novelsjahan_ok\\ready.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Input and Output file\n",
    "input_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\novelsjahan_ok\\ok14.xlsx\"  # ‚Üê apni actual file ka naam yahan likhein\n",
    "output_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\novelsjahan_ok\\ready.xlsx\"\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Remove all dots (.) from 'Title' column\n",
    "df['Title'] = df['Title'].str.replace('.', '', regex=False)\n",
    "\n",
    "# Save the cleaned DataFrame to a new Excel file\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"‚úÖ Dots removed and file saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Phrase removed successfully. File saved as: E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\urdunovelsghar_ok\\ready.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File names\n",
    "input_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\urdunovelsghar_ok\\ok21.xlsx\" # ‚Üê apni file ka naam yahan likhein\n",
    "output_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\urdunovelsghar_ok\\ready.xlsx\"\n",
    "\n",
    "# Load Excel file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Define cleaner function\n",
    "def clean_phrase(title):\n",
    "    if isinstance(title, str):\n",
    "        # Remove the full phrase (case-insensitive)\n",
    "        cleaned = re.sub(r'download pdf complete free of', '', title, flags=re.IGNORECASE)\n",
    "        return re.sub(r'\\s+', ' ', cleaned).strip()  # Remove extra spaces\n",
    "    return title\n",
    "\n",
    "# Apply to 'Title' column\n",
    "df['Title'] = df['Title'].apply(clean_phrase)\n",
    "\n",
    "# Save new Excel file\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"‚úÖ Phrase removed successfully. File saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ '- Urdu Readings' removed successfully. File saved as: E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\urdureadings_ok\\ready.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File names\n",
    "input_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\urdureadings_ok\\ok23.xlsx\" # ‚Üê apni file ka naam yahan likhein\n",
    "output_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\urdureadings_ok\\ready.xlsx\"\n",
    "\n",
    "# Load Excel file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Define function to remove \"- Urdu Readings\" (with optional spaces before/after dash)\n",
    "def clean_urdu_readings(title):\n",
    "    if isinstance(title, str):\n",
    "        cleaned = re.sub(r'\\s*-\\s*Urdu Readings', '', title, flags=re.IGNORECASE)\n",
    "        return cleaned.strip()\n",
    "    return title\n",
    "\n",
    "# Apply cleaning\n",
    "df['Title'] = df['Title'].apply(clean_urdu_readings)\n",
    "\n",
    "# Save cleaned file\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"‚úÖ '- Urdu Readings' removed successfully. File saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ '‚Äì ZNZ' removed successfully. File saved as: E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\zubinovelszone\\ready.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File names\n",
    "input_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\zubinovelszone\\merged_output.xlsx\" # ‚Üê Replace this with your actual file\n",
    "output_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\zubinovelszone\\ready.xlsx\"\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Function to remove '‚Äì ZNZ' (with long dash and optional spaces)\n",
    "def clean_znz(text):\n",
    "    if isinstance(text, str):\n",
    "        cleaned = re.sub(r'\\s*‚Äì\\s*ZNZ', '', text, flags=re.IGNORECASE)\n",
    "        return cleaned.strip()\n",
    "    return text\n",
    "\n",
    "# Apply the function to Title column\n",
    "df['Title'] = df['Title'].apply(clean_znz)\n",
    "\n",
    "# Save the cleaned Excel\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"‚úÖ '‚Äì ZNZ' removed successfully. File saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Excel file saved as: E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\bookkorner\\okkkkkkkkkkk.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from urllib.parse import unquote\n",
    "\n",
    "# Original Excel file\n",
    "input_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\bookkorner\\merged_output.xlsx\"  # Replace with your file name\n",
    "output_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\bookkorner\\okkkkkkkkkkk.xlsx\"\n",
    "\n",
    "# Load the file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Function to extract title from Mediafire link\n",
    "def extract_title_from_link(link):\n",
    "    if isinstance(link, str) and 'mediafire.com' in link:\n",
    "        match = re.search(r'/file/.+?/([^/]+)', link)\n",
    "        if match:\n",
    "            filename = match.group(1)\n",
    "            filename = unquote(filename.replace('+', ' '))  # Decode URL and replace + with space\n",
    "            return filename\n",
    "    return None\n",
    "\n",
    "# Replace Title where Mediafire link exists\n",
    "for idx, row in df.iterrows():\n",
    "    mediafire_link = row.get(\"Mediafire Links\", \"\")\n",
    "    extracted_title = extract_title_from_link(mediafire_link)\n",
    "    if extracted_title:\n",
    "        df.at[idx, \"Title\"] = extracted_title\n",
    "\n",
    "# Save to new Excel file\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"New Excel file saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Input aur output file ka naam\n",
    "input_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\bookkorner\\okkkkkkkkkkk.xlsx\"  # ‚Üê yahan apni file ka naam daalein\n",
    "output_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\bookkorner\\okok.xlsx\"\n",
    "\n",
    "# Excel file read karo\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# 'Title' column mein '.pdf - Google Drive' ko remove karo\n",
    "df['Title'] = df['Title'].str.replace('.pdf', '', case=False).str.strip()\n",
    "\n",
    "# Nayi file save karo\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"‚úÖ Cleaned file saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ '.pdf' and website names removed. File saved as: E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\bookkorner\\olll.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "input_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\bookkorner\\okok.xlsx\" # ‚Üê Replace with your file name\n",
    "output_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\bookkorner\\olll.xlsx\"\n",
    "\n",
    "# Load Excel file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Function to clean '.pdf' and website domains\n",
    "def clean_title(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove .pdf\n",
    "        text = re.sub(r'\\.pdf', '', text, flags=re.IGNORECASE)\n",
    "        # Remove website names (ending with .com, .org, .net etc.)\n",
    "        text = re.sub(r'\\b[\\w.-]+\\.(com|org|net|info|pk|in|us)\\b', '', text, flags=re.IGNORECASE)\n",
    "        # Remove multiple spaces\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "df['Title'] = df['Title'].apply(clean_title)\n",
    "\n",
    "# Save cleaned file\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"‚úÖ '.pdf' and website names removed. File saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All brackets and their content removed. File saved as: E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\bookkorner\\ollllllll.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File names\n",
    "input_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\bookkorner\\olll.xlsx\" # ‚Üê Replace with your file name\n",
    "output_file = r\"E:\\UNB\\Programs\\scrapenovels.py\\Novels excel files\\bookkorner\\ollllllll.xlsx\"\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Function to remove all types of brackets and their content\n",
    "def remove_brackets(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove (), [], {} with content\n",
    "        text = re.sub(r'\\(.*?\\)|\\[.*?\\]|\\{.*?\\}', '', text)\n",
    "        # Clean up extra spaces\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply to Title column\n",
    "df['Title'] = df['Title'].apply(remove_brackets)\n",
    "\n",
    "# Save cleaned file\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"‚úÖ All brackets and their content removed. File saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Agar Title column mein \"DOWNLOAD FREE URDU BOOKS AND NOVELS\" likha ho, to:\n",
    "\n",
    "Usi row ka Mediafire link uthaya jaye\n",
    "\n",
    "Us link se PDF file ka naam extract kiya jaye\n",
    "\n",
    "Us naam mein jitne bhi + symbols hon, unhein space se replace kiya jaye\n",
    "\n",
    "Aur phir \"DOWNLOAD FREE URDU BOOKS AND NOVELS\" ko us cleaned title se replace kar diya jaye\n",
    "\n",
    "Final result ek naye Excel file mein save ho jaye ‚úÖ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned file saved as: E:\\UNB\\oknovels\\mooot1111.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from urllib.parse import unquote\n",
    "\n",
    "# Input and Output files\n",
    "input_file = r\"E:\\UNB\\oknovels\\mooot.xlsx\" # üëà Replace this with your Excel file name\n",
    "output_file = r\"E:\\UNB\\oknovels\\mooot1111.xlsx\"\n",
    "\n",
    "# Load Excel file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Function to extract clean title from mediafire link\n",
    "def clean_title_from_link(link):\n",
    "    if isinstance(link, str) and \"mediafire.com\" in link:\n",
    "        try:\n",
    "            file_part = link.strip().split(\"/\")[-1]\n",
    "            file_part = unquote(file_part)  # decode URL\n",
    "            file_part = file_part.replace(\"+\", \" \")\n",
    "            file_part = re.sub(r'\\.pdf$', '', file_part, flags=re.IGNORECASE)\n",
    "            file_part = re.sub(r'\\burdunovelist\\.blogspot\\.com\\b', '', file_part, flags=re.IGNORECASE)\n",
    "            return file_part.strip()\n",
    "        except:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "# Replace titles with mediafire titles if title has \"DOWNLOAD FREE URDU BOOKS AND NOVELS\"\n",
    "for i, row in df.iterrows():\n",
    "    title = str(row['Title'])\n",
    "    mediafire_link = row.get(\"Mediafire Links\", \"\")\n",
    "    if \"Urdu Books\" in title.upper():\n",
    "        extracted_title = clean_title_from_link(mediafire_link)\n",
    "        if extracted_title:\n",
    "            df.at[i, 'Title'] = extracted_title\n",
    "\n",
    "# Save the cleaned Excel file\n",
    "df.to_excel(output_file, index=False)\n",
    "print(\"‚úÖ Cleaned file saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the Excel file: ['Title', 'Google Drive Links', 'Mediafire Links', 'Links']\n",
      "Filtered data has been saved to E:\\UNB\\oknovels\\mooot.xlsx\n",
      "Original number of rows: 100109\n",
      "Number of rows after filtering: 78293\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the Excel file\n",
    "input_file = r\"E:\\UNB\\oknovels\\merged_output.xlsx\" # Replace with your actual file path\n",
    "try:\n",
    "    df = pd.read_excel(input_file)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {input_file} was not found. Please check the file path.\")\n",
    "    exit()\n",
    "\n",
    "# Step 1.5: Print column names to confirm\n",
    "print(\"Column names in the Excel file:\", df.columns.tolist())\n",
    "\n",
    "# Step 2: Filter out rows where both columns have \"No Google Drive Link\" and \"No Mediafire Link\"\n",
    "try:\n",
    "    filtered_df = df[~((df['Google Drive Links'] == 'No Google Drive Link') & (df['Mediafire Links'] == 'No Mediafire Link'))]\n",
    "except KeyError as e:\n",
    "    print(f\"Error: {e}. One of the columns was not found in the Excel file.\")\n",
    "    print(\"Please check the column names printed above and update the code with the correct column names.\")\n",
    "    exit()\n",
    "\n",
    "# Step 3: Save the filtered data to a new Excel file\n",
    "output_file = r\"E:\\UNB\\oknovels\\mooot.xlsx\" # Replace with your desired output file path\n",
    "filtered_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Filtered data has been saved to {output_file}\")\n",
    "print(f\"Original number of rows: {len(df)}\")\n",
    "print(f\"Number of rows after filtering: {len(filtered_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Excel file 'mooot666.xlsx' has been created with updated links!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the Excel file\n",
    "df = pd.read_excel(r\"E:\\UNB\\oknovels\\mooot.xlsx\")\n",
    "\n",
    "# Step 2: Function to determine the link to use for the \"Links\" column\n",
    "def get_available_link(row):\n",
    "    google_link = row['Google Drive Links']\n",
    "    mediafire_link = row['Mediafire Links']\n",
    "    \n",
    "    # Check if Google Drive link is valid (not \"No Google Drive Link\" or NaN)\n",
    "    if pd.notna(google_link) and \"No Google Drive Link\" not in str(google_link):\n",
    "        return google_link  # Prioritize Google Drive link if available\n",
    "    # If Google Drive link is not available, use Mediafire link if it exists\n",
    "    elif pd.notna(mediafire_link) and \"No Google Drive Link\" not in str(mediafire_link):\n",
    "        return mediafire_link  # Use Mediafire link if Google Drive is not available\n",
    "    else:\n",
    "        return \"No Link Available\"\n",
    "\n",
    "# Step 3: Apply the function to populate the \"Links\" column\n",
    "df['Links'] = df.apply(get_available_link, axis=1)\n",
    "\n",
    "# Step 4: Save the updated dataframe to a new Excel file\n",
    "df.to_excel(r\"E:\\UNB\\oknovels\\mooot666.xlsx\", index=False)\n",
    "\n",
    "print(\"New Excel file 'mooot666.xlsx' has been created with updated links!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Excel file 'mooot666.xlsx' has been created with updated links!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the Excel file\n",
    "df = pd.read_excel(r\"E:\\UNB\\oknovels\\mooot.xlsx\")\n",
    "\n",
    "# Step 2: Function to determine the link to use for the \"Links\" column\n",
    "def get_available_link(row):\n",
    "    google_link = row['Google Drive Links']\n",
    "    mediafire_link = row['Mediafire Links']\n",
    "    \n",
    "    # Check if Google Drive link is valid (not \"No Google Drive Link\" or NaN)\n",
    "    if pd.notna(google_link) and \"No Google Drive Link\" not in str(google_link):\n",
    "        return google_link  # Prioritize Google Drive link if available\n",
    "    # If Google Drive link is not available, use Mediafire link if it exists\n",
    "    elif pd.notna(mediafire_link) and \"No Google Drive Link\" not in str(mediafire_link):\n",
    "        return mediafire_link  # Use Mediafire link if Google Drive is not available\n",
    "    else:\n",
    "        return \"No Link Available\"\n",
    "\n",
    "# Step 3: Apply the function to populate the \"Links\" column\n",
    "df['Links'] = df.apply(get_available_link, axis=1)\n",
    "\n",
    "# Step 4: Function to keep only one link in the \"Links\" column\n",
    "def keep_one_link(links):\n",
    "    # If the links value is not a string or is \"No Link Available\", return as is\n",
    "    if not isinstance(links, str) or links == \"No Link Available\":\n",
    "        return links\n",
    "    \n",
    "    # Split the links by comma\n",
    "    link_list = [link.strip() for link in links.split(\",\")]\n",
    "    \n",
    "    # Look for the first Google Drive link\n",
    "    for link in link_list:\n",
    "        if \"drive.google.com\" in link:\n",
    "            return link\n",
    "    \n",
    "    # If no Google Drive link is found, return the first link\n",
    "    return link_list[0]\n",
    "\n",
    "# Step 5: Apply the function to keep only one link in the \"Links\" column\n",
    "df['Links'] = df['Links'].apply(keep_one_link)\n",
    "\n",
    "# Step 6: Save the updated dataframe to a new Excel file\n",
    "df.to_excel(r\"E:\\UNB\\oknovels\\mooot666.xlsx\", index=False)\n",
    "\n",
    "print(\"New Excel file 'mooot666.xlsx' has been created with updated links!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links column successfully updated and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Excel file ka path\n",
    "file_path = r\"E:\\UNB\\oknovels\\mooot.xlsx\"  # <-- yahan apni Excel file ka actual path den\n",
    "\n",
    "# Excel file load karein\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Links column banayein ya update karein\n",
    "def get_preferred_link(row):\n",
    "    gdrive = row['Google Drive Links']\n",
    "    mediafire = row['Mediafire Links']\n",
    "    \n",
    "    if pd.notna(gdrive) and 'No Google Drive Link' not in str(gdrive):\n",
    "        return gdrive\n",
    "    elif pd.notna(mediafire) and 'No Mediafire Link' not in str(mediafire):\n",
    "        return mediafire\n",
    "    else:\n",
    "        return 'No Link Available'\n",
    "\n",
    "# New column add karein\n",
    "df['Links'] = df.apply(get_preferred_link, axis=1)\n",
    "\n",
    "# Updated Excel file save karein\n",
    "df.to_excel(r\"E:\\UNB\\oknovels\\mooot1.xlsx\", index=False)\n",
    "\n",
    "print(\"Links column successfully updated and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file saved as: E:\\UNB\\oknovels\\moootok.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Excel file ko read karna\n",
    "def update_links_column(excel_file_path, output_file_path):\n",
    "    # Excel file ko pandas DataFrame mein load karna\n",
    "    df = pd.read_excel(excel_file_path)\n",
    "    \n",
    "    # Links column ko initially empty string se fill karna\n",
    "    df['Links'] = ''\n",
    "    \n",
    "    # Har row ke liye Links column ko update karna\n",
    "    for index, row in df.iterrows():\n",
    "        google_drive_link = row['Google Drive Links']\n",
    "        mediafire_link = row['Mediafire Links']\n",
    "        \n",
    "        # Condition 1: Agar dono links available hain to Google Drive link ko prefer karna\n",
    "        if google_drive_link != 'No Google Drive Link' and mediafire_link != 'No Mediafire Link':\n",
    "            df.at[index, 'Links'] = google_drive_link\n",
    "        \n",
    "        # Condition 2: Agar sirf Google Drive link available hai\n",
    "        elif google_drive_link != 'No Google Drive Link' and mediafire_link == 'No Mediafire Link':\n",
    "            df.at[index, 'Links'] = google_drive_link\n",
    "        \n",
    "        # Condition 3: Agar sirf Mediafire link available hai\n",
    "        elif google_drive_link == 'No Google Drive Link' and mediafire_link != 'No Mediafire Link':\n",
    "            df.at[index, 'Links'] = mediafire_link\n",
    "    \n",
    "    # Updated DataFrame ko new Excel file mein save karna\n",
    "    df.to_excel(output_file_path, index=False)\n",
    "    print(f\"Updated file saved as: {output_file_path}\")\n",
    "\n",
    "# File paths\n",
    "input_file = r\"E:\\UNB\\oknovels\\mooot.xlsx\"  # Apni input Excel file ka path\n",
    "output_file = r\"E:\\UNB\\oknovels\\moootok.xlsx\" # Output Excel file ka path\n",
    "\n",
    "# Function call\n",
    "update_links_column(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total titles processed: 78293\n",
      "Titles with missing links: 6884\n",
      "Titles with no links available:\n",
      "- nan\n",
      "- Tere bakhat ki roshani\n",
      "- Phir hua yun\n",
      "- Susral gaidha phool\n",
      "- Jana tere leye\n",
      "- Fasle by Razia But\n",
      "- Hum aur bulbulen complete\n",
      "- sare diye jalte rahe complete\n",
      "- Sagar darya badal boond by Razia Jamil\n",
      "- Ibn e Maryam Novel by Aalia Hira - Zemtime.com\n",
      "... and 6874 more titles\n",
      "Updated file saved as: E:\\UNB\\oknovels\\moootnew.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def update_links_column(excel_file_path, output_file_path):\n",
    "    # Excel file ko pandas DataFrame mein load karna\n",
    "    df = pd.read_excel(excel_file_path)\n",
    "    \n",
    "    # Data ko clean karna: Titles aur links ke columns se extra spaces remove karna\n",
    "    df['Titles'] = df['Titles'].str.strip()\n",
    "    df['Google Drive Links'] = df['Google Drive Links'].str.strip()\n",
    "    df['Mediafire Links'] = df['Mediafire Links'].str.strip()\n",
    "    \n",
    "    # Links column ko initially empty string se fill karna\n",
    "    df['Links'] = ''\n",
    "    \n",
    "    # Missing links ko track karne ke liye counter aur list\n",
    "    missing_links_count = 0\n",
    "    missing_links_titles = []\n",
    "    \n",
    "    # Har row ke liye Links column ko update karna\n",
    "    for index, row in df.iterrows():\n",
    "        google_drive_link = row['Google Drive Links']\n",
    "        mediafire_link = row['Mediafire Links']\n",
    "        \n",
    "        # Check karna ke link columns mein valid data hai ya nahi\n",
    "        has_google_drive = (google_drive_link != 'No Google Drive Link' and \n",
    "                          pd.notna(google_drive_link) and \n",
    "                          google_drive_link != '')\n",
    "        has_mediafire = (mediafire_link != 'No Mediafire Link' and \n",
    "                        pd.notna(mediafire_link) and \n",
    "                        mediafire_link != '')\n",
    "        \n",
    "        # Condition 1: Agar dono links available hain to Google Drive link ko prefer karna\n",
    "        if has_google_drive and has_mediafire:\n",
    "            df.at[index, 'Links'] = google_drive_link\n",
    "        \n",
    "        # Condition 2: Agar sirf Google Drive link available hai\n",
    "        elif has_google_drive and not has_mediafire:\n",
    "            df.at[index, 'Links'] = google_drive_link\n",
    "        \n",
    "        # Condition 3: Agar sirf Mediafire link available hai\n",
    "        elif not has_google_drive and has_mediafire:\n",
    "            df.at[index, 'Links'] = mediafire_link\n",
    "        \n",
    "        # Condition 4: Agar dono links nahi hain to warning dena\n",
    "        else:\n",
    "            missing_links_count += 1\n",
    "            missing_links_titles.append(row['Titles'])\n",
    "            df.at[index, 'Links'] = 'No Link Available'\n",
    "    \n",
    "    # Updated DataFrame ko new Excel file mein save karna\n",
    "    df.to_excel(output_file_path, index=False)\n",
    "    \n",
    "    # Logging: Kitne titles ke links nahi mile aur unke names\n",
    "    print(f\"Total titles processed: {len(df)}\")\n",
    "    print(f\"Titles with missing links: {missing_links_count}\")\n",
    "    if missing_links_titles:\n",
    "        print(\"Titles with no links available:\")\n",
    "        for title in missing_links_titles[:10]:  # Sirf pehle 10 titles print karenge, taake output zyada lambi na ho\n",
    "            print(f\"- {title}\")\n",
    "        if len(missing_links_titles) > 10:\n",
    "            print(f\"... and {len(missing_links_titles) - 10} more titles\")\n",
    "\n",
    "    print(f\"Updated file saved as: {output_file_path}\")\n",
    "\n",
    "# File paths\n",
    "input_file = r\"E:\\UNB\\oknovels\\mooot.xlsx\"   # Apni input Excel file ka path\n",
    "output_file =r\"E:\\UNB\\oknovels\\moootnew.xlsx\"  # Output Excel file ka path\n",
    "\n",
    "# Function call\n",
    "update_links_column(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file ke column names:\n",
      "['Titles', 'Google Drive Links', 'Mediafire Links', 'Links']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Input file path\n",
    "input_file = r\"E:\\UNB\\oknovels\\mooot.xlsx\"\n",
    "\n",
    "# Excel file ko read karna\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Column names print karna\n",
    "print(\"Excel file ke column names:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file ke column names (exact):\n",
      "['Titles', 'Google Drive Links', 'Mediafire Links', 'Links']\n",
      "Total titles processed: 78293\n",
      "Titles with missing links: 6884\n",
      "Titles with no links available:\n",
      "- nan\n",
      "- Tere bakhat ki roshani\n",
      "- Phir hua yun\n",
      "- Susral gaidha phool\n",
      "- Jana tere leye\n",
      "- Fasle by Razia But\n",
      "- Hum aur bulbulen complete\n",
      "- sare diye jalte rahe complete\n",
      "- Sagar darya badal boond by Razia Jamil\n",
      "- Ibn e Maryam Novel by Aalia Hira - Zemtime.com\n",
      "... and 6874 more titles\n",
      "Updated file saved as: E:\\UNB\\oknovels\\moootnew.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def update_links_column(excel_file_path, output_file_path):\n",
    "    try:\n",
    "        # Excel file ko pandas DataFrame mein load karna\n",
    "        df = pd.read_excel(excel_file_path)\n",
    "        \n",
    "        # Column names ko print karna taake confirm kar sakein\n",
    "        print(\"Excel file ke column names (exact):\")\n",
    "        print(df.columns.tolist())\n",
    "        \n",
    "        # Column names ko clean karna (extra spaces remove karna)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        # Check karna ke required columns exist karte hain\n",
    "        required_columns = ['Titles', 'Google Drive Links', 'Mediafire Links']\n",
    "        for col in required_columns:\n",
    "            if col not in df.columns:\n",
    "                raise KeyError(f\"Column '{col}' not found in Excel file. Available columns: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Links column ko add karna agar already nahi hai\n",
    "        if 'Links' not in df.columns:\n",
    "            df['Links'] = ''\n",
    "        \n",
    "        # Data ko clean karna: Titles aur links ke columns se extra spaces remove karna\n",
    "        df['Titles'] = df['Titles'].str.strip()\n",
    "        df['Google Drive Links'] = df['Google Drive Links'].str.strip()\n",
    "        df['Mediafire Links'] = df['Mediafire Links'].str.strip()\n",
    "        \n",
    "        # Missing links ko track karne ke liye counter aur list\n",
    "        missing_links_count = 0\n",
    "        missing_links_titles = []\n",
    "        \n",
    "        # Har row ke liye Links column ko update karna\n",
    "        for index, row in df.iterrows():\n",
    "            google_drive_link = row['Google Drive Links']\n",
    "            mediafire_link = row['Mediafire Links']\n",
    "            \n",
    "            # Check karna ke link columns mein valid data hai ya nahi\n",
    "            has_google_drive = (google_drive_link != 'No Google Drive Link' and \n",
    "                              pd.notna(google_drive_link) and \n",
    "                              google_drive_link != '')\n",
    "            has_mediafire = (mediafire_link != 'No Mediafire Link' and \n",
    "                            pd.notna(mediafire_link) and \n",
    "                            mediafire_link != '')\n",
    "            \n",
    "            # Condition 1: Agar dono links available hain to Google Drive link ko prefer karna\n",
    "            if has_google_drive and has_mediafire:\n",
    "                df.at[index, 'Links'] = google_drive_link\n",
    "            \n",
    "            # Condition 2: Agar sirf Google Drive link available hai\n",
    "            elif has_google_drive and not has_mediafire:\n",
    "                df.at[index, 'Links'] = google_drive_link\n",
    "            \n",
    "            # Condition 3: Agar sirf Mediafire link available hai\n",
    "            elif not has_google_drive and has_mediafire:\n",
    "                df.at[index, 'Links'] = mediafire_link\n",
    "            \n",
    "            # Condition 4: Agar dono links nahi hain to warning dena\n",
    "            else:\n",
    "                missing_links_count += 1\n",
    "                missing_links_titles.append(row['Titles'])\n",
    "                df.at[index, 'Links'] = 'No Link Available'\n",
    "        \n",
    "        # Updated DataFrame ko new Excel file mein save karna\n",
    "        df.to_excel(output_file_path, index=False)\n",
    "        \n",
    "        # Logging: Kitne titles ke links nahi mile aur unke names\n",
    "        print(f\"Total titles processed: {len(df)}\")\n",
    "        print(f\"Titles with missing links: {missing_links_count}\")\n",
    "        if missing_links_titles:\n",
    "            print(\"Titles with no links available:\")\n",
    "            for title in missing_links_titles[:10]:  # Sirf pehle 10 titles print karenge\n",
    "                print(f\"- {title}\")\n",
    "            if len(missing_links_titles) > 10:\n",
    "                print(f\"... and {len(missing_links_titles) - 10} more titles\")\n",
    "\n",
    "        print(f\"Updated file saved as: {output_file_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "\n",
    "# File paths\n",
    "input_file = r\"E:\\UNB\\oknovels\\mooot.xlsx\"  # Input Excel file ka path\n",
    "output_file = r\"E:\\UNB\\oknovels\\moootnew.xlsx\"  # Output Excel file ka path\n",
    "\n",
    "# Function call\n",
    "update_links_column(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file ke column names (exact):\n",
      "['Titles', 'Google Drive Links', 'Mediafire Links', 'Links']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PCS\\AppData\\Local\\Temp\\ipykernel_3860\\1538312945.py:53: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'https://drive.google.com/file/d/1d34nWpBlEZmRsAMSKcqI8EXbKkwFyu26/view?usp=sharing, https://drive.google.com/file/d/1d34nWpBlEZmRsAMSKcqI8EXbKkwFyu26/view?usp=sharing' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[index, 'Links'] = google_drive_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total titles processed before deletion: 71410\n",
      "Titles with missing links: 1\n",
      "Titles with no links available (will be deleted):\n",
      "- nan\n",
      "Total titles after deletion: 71409\n",
      "Updated file saved as: E:\\UNB\\oknovels\\moootnew.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def update_links_column(excel_file_path, output_file_path):\n",
    "    try:\n",
    "        # Excel file ko pandas DataFrame mein load karna\n",
    "        df = pd.read_excel(excel_file_path)\n",
    "        \n",
    "        # Column names ko print karna taake confirm kar sakein\n",
    "        print(\"Excel file ke column names (exact):\")\n",
    "        print(df.columns.tolist())\n",
    "        \n",
    "        # Column names ko clean karna (extra spaces remove karna)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        # Check karna ke required columns exist karte hain\n",
    "        required_columns = ['Titles', 'Google Drive Links', 'Mediafire Links']\n",
    "        for col in required_columns:\n",
    "            if col not in df.columns:\n",
    "                raise KeyError(f\"Column '{col}' not found in Excel file. Available columns: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Links column ko add karna agar already nahi hai\n",
    "        if 'Links' not in df.columns:\n",
    "            df['Links'] = ''\n",
    "        \n",
    "        # Data ko clean karna: Titles aur links ke columns se extra spaces remove karna\n",
    "        df['Titles'] = df['Titles'].str.strip()\n",
    "        df['Google Drive Links'] = df['Google Drive Links'].str.strip()\n",
    "        df['Mediafire Links'] = df['Mediafire Links'].str.strip()\n",
    "        \n",
    "        # Missing links ko track karne ke liye counter aur list\n",
    "        missing_links_count = 0\n",
    "        missing_links_titles = []\n",
    "        \n",
    "        # Har row ke liye Links column ko update karna\n",
    "        for index, row in df.iterrows():\n",
    "            google_drive_link = row['Google Drive Links']\n",
    "            mediafire_link = row['Mediafire Links']\n",
    "            \n",
    "            # Check karna ke link columns mein valid data hai ya nahi\n",
    "            has_google_drive = (google_drive_link != 'No Google Drive Link' and \n",
    "                              pd.notna(google_drive_link) and \n",
    "                              google_drive_link != '')\n",
    "            has_mediafire = (mediafire_link != 'No Mediafire Link' and \n",
    "                            pd.notna(mediafire_link) and \n",
    "                            mediafire_link != '')\n",
    "            \n",
    "            # Condition 1: Agar dono links available hain to Google Drive link ko prefer karna\n",
    "            if has_google_drive and has_mediafire:\n",
    "                df.at[index, 'Links'] = google_drive_link\n",
    "            \n",
    "            # Condition 2: Agar sirf Google Drive link available hai\n",
    "            elif has_google_drive and not has_mediafire:\n",
    "                df.at[index, 'Links'] = google_drive_link\n",
    "            \n",
    "            # Condition 3: Agar sirf Mediafire link available hai\n",
    "            elif not has_google_drive and has_mediafire:\n",
    "                df.at[index, 'Links'] = mediafire_link\n",
    "            \n",
    "            # Condition 4: Agar dono links nahi hain to warning dena\n",
    "            else:\n",
    "                missing_links_count += 1\n",
    "                missing_links_titles.append(row['Titles'])\n",
    "                df.at[index, 'Links'] = 'No Link Available'\n",
    "        \n",
    "        # Logging: Kitne titles ke links nahi mile aur unke names\n",
    "        print(f\"Total titles processed before deletion: {len(df)}\")\n",
    "        print(f\"Titles with missing links: {missing_links_count}\")\n",
    "        if missing_links_titles:\n",
    "            print(\"Titles with no links available (will be deleted):\")\n",
    "            for title in missing_links_titles[:10]:  # Sirf pehle 10 titles print karenge\n",
    "                print(f\"- {title}\")\n",
    "            if len(missing_links_titles) > 10:\n",
    "                print(f\"... and {len(missing_links_titles) - 10} more titles\")\n",
    "        \n",
    "        # \"No Link Available\" wali rows ko delete karna\n",
    "        df = df[df['Links'] != 'No Link Available']\n",
    "        \n",
    "        # Updated DataFrame ko new Excel file mein save karna\n",
    "        df.to_excel(output_file_path, index=False)\n",
    "        \n",
    "        # Final logging\n",
    "        print(f\"Total titles after deletion: {len(df)}\")\n",
    "        print(f\"Updated file saved as: {output_file_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "\n",
    "# File paths\n",
    "input_file = r\"E:\\UNB\\oknovels\\mooot.xlsx\"  # Input Excel file ka path\n",
    "output_file = r\"E:\\UNB\\oknovels\\moootnew.xlsx\"  # Output Excel file ka path\n",
    "\n",
    "# Function call\n",
    "update_links_column(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File processed successfully! Check the output at: E:\\UNB\\oknovels\\okkkkkk - Copyokkk.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define input and output file paths\n",
    "input_file_path = r\"E:\\UNB\\oknovels\\okkkkkk - Copy.xlsx\" # Replace with your input file path\n",
    "output_file_path = r\"E:\\UNB\\oknovels\\okkkkkk - Copyokkk.xlsx\" # Replace with your desired output file path\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(input_file_path)\n",
    "\n",
    "# Function to keep only the first link if there are multiple links\n",
    "def keep_first_link(links):\n",
    "    if isinstance(links, str):  # Check if the value is a string\n",
    "        link_list = links.split(\",\")  # Split by comma\n",
    "        if len(link_list) > 1:  # If more than one link\n",
    "            return link_list[0].strip()  # Keep the first link and remove extra spaces\n",
    "    return links  # Return as is if there's only one link or it's not a string\n",
    "\n",
    "# Apply the function to the \"Links\" column\n",
    "df[\"Links\"] = df[\"Links\"].apply(keep_first_link)\n",
    "\n",
    "# Save the modified dataframe to a new Excel file\n",
    "df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f\"File processed successfully! Check the output at: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Har title ko check karo\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m title \u001b[38;5;129;01min\u001b[39;00m df[\u001b[33m'\u001b[39m\u001b[33mTitles\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# \"by\" ke baad ka text extract karna (case insensitive)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     match = \u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mbby\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mb\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43ms+(.*)\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43mIGNORECASE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m match:\n\u001b[32m     18\u001b[39m         authors.append(match.group(\u001b[32m1\u001b[39m).strip())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PCS\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\re\\__init__.py:177\u001b[39m, in \u001b[36msearch\u001b[39m\u001b[34m(pattern, string, flags)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msearch\u001b[39m(pattern, string, flags=\u001b[32m0\u001b[39m):\n\u001b[32m    175\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Scan through string looking for a match to the pattern, returning\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[33;03m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: expected string or bytes-like object, got 'float'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Excel file ka path den\n",
    "input_file = r\"E:\\UNB\\oknovels\\okkkkkk - Copyokkk.xlsx\"\n",
    "output_file = r\"E:\\UNB\\oknovels\\authors.xlsx\"\n",
    "# Excel file read karo\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Authors ko store karne ke liye list\n",
    "authors = []\n",
    "\n",
    "# Har title ko check karo\n",
    "for title in df['Titles']:\n",
    "    # \"by\" ke baad ka text extract karna (case insensitive)\n",
    "    match = re.search(r'\\bby\\b\\s+(.*)', title, re.IGNORECASE)\n",
    "    if match:\n",
    "        authors.append(match.group(1).strip())\n",
    "\n",
    "# Authors list ko DataFrame mein convert karo\n",
    "authors_df = pd.DataFrame({'Author': authors})\n",
    "\n",
    "# New Excel file save karo\n",
    "authors_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Total {len(authors)} authors extracted and saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 53755 authors to E:\\UNB\\oknovels\\authors.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "input_file = r\"E:\\UNB\\oknovels\\okkkkkk - Copyokkk.xlsx\"\n",
    "output_file = r\"E:\\UNB\\oknovels\\authors.xlsx\"\n",
    "\n",
    "# Read Excel file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Authors list\n",
    "authors = []\n",
    "\n",
    "# Check each title\n",
    "for title in df['Titles']:\n",
    "    if isinstance(title, str):  # Only apply regex if title is a string\n",
    "        match = re.search(r'\\bby\\b\\s+(.*)', title, re.IGNORECASE)\n",
    "        if match:\n",
    "            authors.append(match.group(1).strip())\n",
    "\n",
    "# Save authors to Excel\n",
    "authors_df = pd.DataFrame({'Author': authors})\n",
    "authors_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Extracted {len(authors)} authors to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19891 unique authors with titles saved to E:\\UNB\\oknovels\\authors.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "input_file = r\"E:\\UNB\\oknovels\\okkkkkk - Copyokkk.xlsx\"\n",
    "output_file = r\"E:\\UNB\\oknovels\\authors.xlsx\"\n",
    "\n",
    "# Read Excel file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# List to hold (Title, Author) pairs\n",
    "data = []\n",
    "\n",
    "# Go through each title\n",
    "for title in df['Titles']:\n",
    "    if isinstance(title, str):\n",
    "        match = re.search(r'\\bby\\b\\s+(.*)', title, re.IGNORECASE)\n",
    "        if match:\n",
    "            author = match.group(1).strip()\n",
    "            data.append((title, author))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_authors = pd.DataFrame(data, columns=['Title', 'Author'])\n",
    "\n",
    "# Remove duplicate authors (keeping first occurrence)\n",
    "df_unique = df_authors.drop_duplicates(subset='Author')\n",
    "\n",
    "# Save to Excel\n",
    "df_unique.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"{len(df_unique)} unique authors with titles saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19891 unique authors sorted and saved to E:\\UNB\\oknovels\\authors.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "input_file = r\"E:\\UNB\\oknovels\\okkkkkk - Copyokkk.xlsx\"\n",
    "output_file = r\"E:\\UNB\\oknovels\\authors.xlsx\"\n",
    "\n",
    "# Read Excel file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# List to hold (Title, Author) pairs\n",
    "data = []\n",
    "\n",
    "# Go through each title\n",
    "for title in df['Titles']:\n",
    "    if isinstance(title, str):\n",
    "        match = re.search(r'\\bby\\b\\s+(.*)', title, re.IGNORECASE)\n",
    "        if match:\n",
    "            author = match.group(1).strip()\n",
    "            data.append((title, author))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_authors = pd.DataFrame(data, columns=['Title', 'Author'])\n",
    "\n",
    "# Remove duplicate authors and sort alphabetically\n",
    "df_unique = df_authors.drop_duplicates(subset='Author')\n",
    "df_unique = df_unique.sort_values(by='Author')  # <-- Sorting here\n",
    "\n",
    "# Save to Excel\n",
    "df_unique.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"{len(df_unique)} unique authors sorted and saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
